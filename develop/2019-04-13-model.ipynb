{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intorduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is in this notebook?\n",
    "1. Exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the inputs which the model needs to run, please select one of the below for each input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs go here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magics & Versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below table shows the version of libraries and packages used for running the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inline matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "# Interactive matplotlib plot()\n",
    "#%matplotlib notebook\n",
    "\n",
    "# Autoreload packages before runs\n",
    "# https://ipython.org/ipython-doc/dev/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# %install_ext http://raw.github.com/jrjohansson/version_information/master/version_information.py\n",
    "# ~/anaconda/bin/pip install version_information\n",
    "%load_ext version_information\n",
    "%version_information numpy, scipy, matplotlib, pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "\n",
    "# Third party imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Date and time\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# Ipython imports\n",
    "from IPython.display import FileLink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn imports\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import Imputer, LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer, StandardScaler\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# pandas plots\n",
    "from pandas.plotting import scatter_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customizations\n",
    "sns.set() # matplotlib defaults\n",
    "\n",
    "# Any tweaks that normally go in .matplotlibrc, etc., should explicitly go here\n",
    "plt.rcParams['figure.figsize'] = (12, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the notebook the saved figures came from\n",
    "fig_prefix = \"../figures/2019-04-13-mh-data-exploration\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data downloading\n",
    "Data was downloaded using the notebook below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileLink('2019-04-10-mh-data-downloading.ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "housing = pd.read_csv('../data/training/housing.csv')\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "1. The dataset size is 20640, which is relatively small.\n",
    "2. The total_bedrooms has only 20433, which means that there are some missing data which should be taken care of.\n",
    "3. All attributes are numerical except for the ocean_proximity which means it is probably a categorical feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the categorical feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.ocean_proximity.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking other fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting histograms for numerical features\n",
    "housing.hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "1. thedata has been scaled and capped at 15 (actually 15.0001) for higher medianincomes, and at 0.5 (actually 0.4999) for lower median incomes.\n",
    "2. The housing median age and the median house value were also capped. The lattermay be a serious problem since it is your target attribute (your labels). The Machine Learning algorithms may learn that prices never go beyond that limit. Check with your client team (the team that will use the systemâ€™s output) to see if this is a problem or not. If they tell that they need precise predictions even beyond \\$500,000, then you have mainly two options:\n",
    "    1. Collect proper labels for the districts whose labels were capped.\n",
    "    2. Remove those districts from the training set (and also from the test set, since your system should not be evaluated poorly if it predicts values beyond\n",
    "\\$500,000).\n",
    "3. The attributes have very different scales. Scalling will be required.\n",
    "4. Finally, many histograms are tail heavy: they extend much farther to the right of\n",
    "the median than to the left. This may make it a bit harder for some Machine Learning algorithms to detect patterns. We will try transforming these attributes\n",
    "later on to have more bell-shaped distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data snooping bias\n",
    "Creating a test set that early is good to void _*data snooping bias*_ where your brain might recognize patterns in the test set and start overfitting those patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42) \n",
    "print(len(train_set), \"train +\", len(test_set), \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The median income is a very important feature for predicting the median housing price.\n",
    "Thus, we will need statified sampling for the train test split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the median_income histogram, most values are clustered around 2-5 but some go far beyond 6.\n",
    "\n",
    "The following code creates an income category attribute by dividing the median income by 1.5 (to limit the number of income categories), and rounding up using ceil (to have discrete categories), and then merging all the categories greater than 5 into category 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating income_cat column\n",
    "housing['income_cat'] = np.ceil(housing['median_income'] / 1.5)\n",
    "housing['income_cat'].where(housing['income_cat'] < 5, 5.0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the split\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing['income_cat']):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the income_cat folumn\n",
    "for set in (strat_train_set, strat_test_set):\n",
    "    set.drop(['income_cat'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a copy of the stratified training data\n",
    "housing = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geographical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting latitude and longitude\n",
    "housing.plot(kind='scatter', x='longitude', y='latitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing alpha to look at areas of high density\n",
    "housing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding more visualization\n",
    "# size -> poplulation\n",
    "# color -> price\n",
    "housing.plot(kind='scatter',\n",
    "             x='longitude', \n",
    "             y='latitude',\n",
    "             s=housing.population/100,\n",
    "             label='population',\n",
    "             c='median_house_value',\n",
    "             cmap=plt.get_cmap('jet'),\n",
    "             colorbar=True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking for correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a correlation matrix \n",
    "corr_matrix = housing.corr()\n",
    "corr_matrix.median_house_value.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting correlations\n",
    "# using pandas scatter matrix\n",
    "attributes = ['median_house_value', 'median_income',\n",
    "              'total_rooms', 'housing_median_age']\n",
    "scatter_matrix(housing[attributes],  figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# further investigation for the median income vs the median house value\n",
    "housing.plot(kind='scatter', x='median_income', y='median_house_value',\n",
    "             alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "1. There is a strong correlation between the median house price and the median income.\n",
    "2. There is a clear horizontal line at \\$500,000 because of the cap.\n",
    "3. There is also few other horizonral lines at \\$450,000 \\$350,000 \\$280,000 and few more. We may need to remove these districts to prevent the algorithm from learning to reproduce these data quirks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribute combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting new attributes \n",
    "housing['rooms_per_household'] = housing['total_rooms']/housing['households']\n",
    "housing['bedrooms_per_room'] =  housing['total_bedrooms']/housing['total_rooms']\n",
    "housing['population_per_household'] = housing['population']/housing['households']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking correlations\n",
    "corr_matrix = housing.corr()\n",
    "corr_matrix['median_house_value'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new bedrooms_per_room attribute is much more correlated with\n",
    "the median house value than the total number of rooms or bedrooms. Apparently houses with a lower bedroom/room ratio tend to be more expensive. The number of rooms per household is also more informative than the total number of rooms in a districtâ€”obviously the larger the houses, the more expensive they are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data to predictors and targets\n",
    "housing = strat_train_set.drop('median_house_value', axis=1)\n",
    "housing_labels = strat_train_set['median_house_value'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting rid of NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 options:\n",
    "1. Get rid of the corresponding districts. (row)\n",
    "2. Get rid of the whole attribute. (column)\n",
    "3. Set the values to some value (zero, mean, median, most common, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "option 1\n",
    "```python\n",
    "housing.dropna(subset=['total_bedrooms'])\n",
    "```\n",
    "\n",
    "option 2\n",
    "```python\n",
    "housing.drop('total_bedrooms', axis=1)\n",
    "```\n",
    "option 3\n",
    "```python\n",
    "median = housing['total_bedrooms'].median()\n",
    "housing['total_bedrooms'].fillna(median)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The median should be stored because the same value will be used for the test set and all new data as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using sklearn imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiating an imputer\n",
    "imputer = Imputer(strategy='median')\n",
    "\n",
    "# removing categorical fetures\n",
    "housing_num = housing.drop('ocean_proximity', axis=1)\n",
    "\n",
    "# fitting the imputer\n",
    "imputer.fit(housing_num)\n",
    "\n",
    "# Checing the imputer\n",
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the numerical features\n",
    "X = imputer.transform(housing_num)\n",
    "\n",
    "# putting the data in a dataframe\n",
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling text and categorical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding categorical features\n",
    "encoder = LabelEncoder()\n",
    "housing_cat = housing['ocean_proximity']\n",
    "housing_cat_encoded = encoder.fit_transform(housing_cat)\n",
    "housing_cat_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the one hot encoder\n",
    "encoder = OneHotEncoder()\n",
    "housing_cat_1hot = encoder.fit_transform(housing_cat_encoded.reshape(-1, 1))\n",
    "housing_cat_1hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the Label Binarizer instead\n",
    "encoder = LabelBinarizer(sparse_output=True)\n",
    "housing_cat_1hot = encoder.fit_transform(housing_cat)\n",
    "housing_cat_1hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although Scikit-Learn provides many useful transformers, you will need to write\n",
    "your own for tasks such as custom cleanup operations or combining specific\n",
    "attributes. You will want your transformer to work seamlessly with Scikit-Learn functionalities\n",
    "(such as pipelines), and since Scikit-Learn relies on duck typing (not inheritance),\n",
    "all you need is to create a class and implement three methods: fit()\n",
    "(returning self), transform(), and fit_transform(). You can get the last one for\n",
    "free by simply adding TransformerMixin as a base class. Also, if you add BaseEstima\n",
    "tor as a base class (and avoid *args and **kargs in your constructor) you will get\n",
    "two extra methods (get_params() and set_params()) that will be useful for automatic\n",
    "hyperparameter tuning. For example, here is a small transformer class that adds\n",
    "the combined attributes we discussed earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "rooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self # nothing else to do\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, household_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, household_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, \n",
    "                         population_per_household, bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying the previous class\n",
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
    "housing_extra_attributes = attr_adder.transform(housing.values)\n",
    "housing_extra_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation piplines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names=attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting lists of numerical and categorical features\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = ['ocean_proximity']\n",
    "\n",
    "# Numerical attributes pipeline\n",
    "num_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector(num_attribs)),\n",
    "    ('imputer', Imputer(strategy='median')),\n",
    "    ('attribs_adder', CombinedAttributesAdder()),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "# Categorical attributes pipeline\n",
    "cat_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector(cat_attribs)),\n",
    "    ('label_binarizer', LabelBinarizer()),\n",
    "])\n",
    "\n",
    "# Full pipeline\n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "    ('num_pipeline', num_pipeline),\n",
    "    ('cat_pipeline', cat_pipeline),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the whole pipeline\n",
    "housing_prepared = full_pipeline.fit_transform(housing)\n",
    "housing_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a list of strings that can be passed to the scoring parameter:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to display the scores\n",
    "def display_scores(scores):\n",
    "    print('Scores:', scores)\n",
    "    print('Mean:', scores.mean())\n",
    "    print('Standard deviation:', scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "lin_rmse_scores = np.sqrt(-lin_scores)\n",
    "display_scores(lin_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_scores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "tree_rmse_scores = np.sqrt(-tree_scores)\n",
    "display_scores(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_reg = RandomForestRegressor()\n",
    "forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "forest_rmse_scores = np.sqrt(-forest_scores)\n",
    "display_scores(forest_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickling the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(my_model, 'my_model.pkl')\n",
    "# and later...\n",
    "my_model = joblib.load('my_model.pkl')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest model seems to be giving the best results. So, I will try to fine-tune that model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "]\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error')\n",
    "\n",
    "grid_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best params\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TODO:__ Try higher n_estimators since 30 is the best and 30 was the last in that list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best estimator\n",
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation scores\n",
    "cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres['mean_test_score'], cvres['params']):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized Search\n",
    "to be tested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importances\n",
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display attributes and their importances\n",
    "extra_attribs = ['rooms_per_hhold', 'pop_per_hhold', 'bedrooms_per_room']\n",
    "cat_one_hot_attribs = list(encoder.classes_)\n",
    "attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
    "sorted(zip(feature_importances, attributes), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this information, we may need to drop som eless useful features such as 'NEAR OCEAN' and 'NEAR BA'.\n",
    "\n",
    "We should also look at the specific errors that your system makes, then try to understand\n",
    "why it makes them and what could fix the problem (adding extra features or, on\n",
    "the contrary, getting rid of uninformative ones, cleaning up outliers, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "X_test = strat_test_set.drop('median_house_value', axis=1)\n",
    "y_test = strat_test_set['median_house_value'].copy()\n",
    "\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "\n",
    "final_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
